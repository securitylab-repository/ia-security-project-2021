{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1+"
    },
    "colab": {
      "name": "projetiasec2021-notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/securitylab-repository/ia-security-project-2021/blob/main/projetiasec2021_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmJnhLsad5Qw"
      },
      "source": [
        "# Projet AI & Security - 2021\n",
        "\n",
        "## Part 1\n",
        "\n",
        "### 1. \n",
        "<img src='https://drive.google.com/uc?id=17QDSJ6k0ANSKEReSUUYG99WEE4DdILfI' alt=\"50%\" style=\"zoom:20%\" />\n",
        "\n",
        "As we have seen in the class, the linear classifiers such as perceptron and SVM work only if the dataset is linearly separable like the image above.\n",
        "\n",
        "However, there are solutions using concepts called basic expansion and kernel that extend these linear classifiers to be efficient and pratical in the case where data are not linearly separable, they admit instead a quadratic or polynomial separator (cf. images below).\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1BJ99omnZMtaFDW1_iXROEaeWx7KQFg3z' alt=\"50%\" style=\"zoom:20%\" />\n",
        "\n",
        "![](https://drive.google.com/uc?id=1fNsV8VR4DvXnXQNNwQRu8DqPixprn3D6)\n",
        "\n",
        "\n",
        "> See these videos\n",
        "\n",
        "- [video 1](https://efrei365net-my.sharepoint.com/:v:/g/personal/boussad_aitsalem_efrei_net/Ec_sBy1Rzq5JuI_kcoDoia0BAdNvbf7puoawodRgTCWFGQ?e=8dFfXe)\n",
        "- [video 2](https://efrei365net-my.sharepoint.com/:v:/g/personal/boussad_aitsalem_efrei_net/Ec_sBy1Rzq5JuI_kcoDoia0BAdNvbf7puoawodRgTCWFGQ?e=w4D8HC)\n",
        "- [video 3](https://efrei365net-my.sharepoint.com/:v:/g/personal/boussad_aitsalem_efrei_net/EfhSuex2aqlKnhE4TAU-PHQBwCcPJdPupeIEpQW4ZyWlHg?e=cHwbX1)\n",
        "- [video 4](https://efrei365net-my.sharepoint.com/:v:/g/personal/boussad_aitsalem_efrei_net/EeeVAcBUz5VEj9zsUobX_ZMBotaaKI5sssw0_M-1KdLRyQ?e=vhFaCP)\n",
        "\n",
        "and make a presentation explaining how to go from linear perceptron/SVM algorithm to the non-linear one.\n",
        "\n",
        "###2. \n",
        "\n",
        "We have discussed in class, the case of binary classification. However, in practice we can face to multi-class learning problem. In this context, give the differences between:\n",
        "\n",
        "  - The One-vs-Rest strategy\n",
        "  - The One-vs-One strategy \n",
        "\n",
        "## Part 2\n",
        "\n",
        "In this section you will try to build a network attack classifier from scratch using machine learning. The dataset that we will use is the [NSL-KDD dataset](https://www.unb.ca/cic/datasets/nsl.html), which is an improvement to a classic network intrusion detection dataset used widely by security data science professionals. The original [1999 KDD Cup dataset](https://kdd.ics.uci.edu/databases/kddcup99/task.html) was created for the DARPA Intrusion Detection Evaluation Program, prepared and managed by MIT Lincoln Laboratory.\n",
        "\n",
        "The data was collected over nine weeks and consists of raw tcpdump traffic in a local area network (LAN) that simulates the environment of a typical United States Air Force LAN. Some network attacks were deliberately carried out during the recording period. There were 38 different types of attacks, but only 24 are available in the training set. These attacks belong to four general categories:\n",
        "\n",
        "- dos Denial of service\n",
        "-  r2l Unauthorized accesses from remote servers\n",
        "-  u2r Privilege escalation attempts\n",
        "- probe Brute-force probing attacks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kiqr_KRLfLok"
      },
      "source": [
        "### Exploring the Data\n",
        "\n",
        "Let’s begin by getting more intimate with the data on hand. The labeled training data as comma-separated values (CSV) looks like this:\n",
        "```\n",
        "0,tcp,ftp_data,SF,491,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0.00,0.00,\n",
        "0.00,0.00,1.00,0.00,0.00,150,25,0.17,0.03,0.17,0.00,0.00,0.00,0.05,\n",
        "0.00,normal,20\n",
        "```\n",
        "```\n",
        "0,icmp,eco_i,SF,8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,21,0.00,0.00,\n",
        "0.00,0.00,1.00,0.00,1.00,2,60,1.00,0.00,1.00,0.50,0.00,0.00,0.00,\n",
        "0.00,ipsweep,17 \n",
        "```\n",
        "\n",
        "The last value in each record is an artifact of the NSL-KDD improvement that we can ignore. The class label is the second-to-last value in each record, and the other 41 values correspond to these features:\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1gw43YWxXRbDC2PHqIV2v_Wsv0GxylnG8' alt=\"50%\" style=\"zoom:20%;\" />\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1L1ItB_4j1wYXhyvjCVibs25mMBq8eOU5' alt=\"50%\" style=\"zoom:20%;\" />\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1BuM-twCi4hgKKPg63ULjjtgWNaQqurSQ' alt=\"50%\" style=\"zoom:20%;\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAcHXsaOdTJG"
      },
      "source": [
        "## Reading and processing dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9ejcxXOdTJB"
      },
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLxn3vZ8dTJG"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MyOXotFdTJH"
      },
      "source": [
        "dataset_root = 'datasets/nsl-kdd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnMn4xPMdTJH"
      },
      "source": [
        "train_file = os.path.join(dataset_root, 'KDDTrain+.txt')\n",
        "test_file = os.path.join(dataset_root, 'KDDTest+.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ESccaahdTJH"
      },
      "source": [
        "# header_names is a list of feature names in the same order as the data\n",
        "header_names = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'attack_type', 'success_pred']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYSRac8C_1lT"
      },
      "source": [
        "Our task is to devise a general classifier that categorizes each individual sample as one of five classes: `benign`, `dos`, `r2l`, `u2r`, or `probe`. The training dataset contains samples that are labeled with the specific attack: `ftp_write` and `guess_passwd` attacks correspond to the `r2l` category, `smurf` and `udpstorm` correspond to the `dos` category, and so on. The mapping from attack labels to attack categories is specified in the file `training_attack_types.txt`\n",
        "\n",
        "Thnaks to the  following code we find that there are 41 attack types specified. Each of them belonging to some category : `benign`, `dos`, `r2l`, `u2r`, or `probe`. The mapping attack types and catorgy is saved in `attack_mapping` variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUMxaX_HdTJI"
      },
      "source": [
        "# training_attack_types.txt maps each of the 22 different attacks to 1 of 4 categories\n",
        "# file obtained from http://kdd.ics.uci.edu/databases/kddcup99/training_attack_types\n",
        "\n",
        "category = defaultdict(list)\n",
        "category['benign'].append('normal')\n",
        "\n",
        "with open('datasets/training_attack_types.txt', 'r') as f:\n",
        "    for line in f.readlines():\n",
        "        attack, cat = line.strip().split(' ')\n",
        "        category[cat].append(attack)\n",
        "\n",
        "attack_mapping = dict((v,k) for k in category for v in category[k])\n",
        "print(attack_mapping)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEb7ZEqbEIUV"
      },
      "source": [
        "> Starting from this point, create training and test set by:\n",
        "  - adding a new feature `attack_category` representing each category. \n",
        "  - removing unnecessary feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5nkwGumdTJM"
      },
      "source": [
        "## Generating and analyzing train and test sets\n",
        "\n",
        "> In order to understand the two data set, let’s **compute** and **plot** the `attack_type` and `attack_category` distributions.\n",
        "\n",
        "> How do you note ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUtLdHA7dTJP"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "henYdcC7dTJQ"
      },
      "source": [
        "> Split the **test** and **training** DataFrames into **data (x)** and **labels (y)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch0ms76T44vq"
      },
      "source": [
        "The following code use [kddcup.names](http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names) to separate between nominal (categorical), binary and numerical (continous) features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLZni01RdTJH"
      },
      "source": [
        "# Differentiating between nominal, binary, and numeric features\n",
        "\n",
        "# root_shell is marked as a continuous feature in the kddcup.names \n",
        "# file, but it is supposed to be a binary feature according to the \n",
        "# dataset documentation\n",
        "\n",
        "col_names = np.array(header_names)\n",
        "\n",
        "nominal_idx = [1, 2, 3]\n",
        "binary_idx = [6, 11, 13, 14, 20, 21]\n",
        "numeric_idx = list(set(range(41)).difference(nominal_idx).difference(binary_idx))\n",
        "\n",
        "nominal_cols = col_names[nominal_idx].tolist()\n",
        "binary_cols = col_names[binary_idx].tolist()\n",
        "numeric_cols = col_names[numeric_idx].tolist()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmqbcrpuS2ih"
      },
      "source": [
        "> Use the function `pd.get_dummies()`that applies [one-hot encoding](https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding) to categorical (nominal) variables such as flag, creating multiple binary variables for each possible value of flag that appears in the dataset. For instance, if a sample has value flag=S2, its dummy variable representation (for flag) will be: \n",
        "\n",
        "```\n",
        "# flag_S0, flag_S1, flag_S2, flag_S3, flag_SF, flag_SH\n",
        "[    0,       0,       1,       0,       0,       0    ]\n",
        "```\n",
        "\n",
        "> For each sample, only one of these variables can have the value 1; hence the name “one-hot.”\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnDbN-_GT16T"
      },
      "source": [
        "The following code gives us the descriptive statistics for the `duration` feature. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEOrMUQ8dTJQ"
      },
      "source": [
        "# Example statistics for the 'duration' feature before scaling\n",
        "train_x['duration'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kj2_Y_fJUytd"
      },
      "source": [
        "> Extend the code to all continous features\n",
        "\n",
        "> Is the dataset standardized or normalized ? If no, solve the problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkcLt4gKcMLz"
      },
      "source": [
        "### Classification\n",
        "\n",
        "It's time now to start the training and test step. For each algorithm SVM, Knn and decision-Tree:\n",
        "- Train a Model\n",
        "- Use cross validation process\n",
        "to set the algorithm parameters\n",
        "- Compute the error rate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcXIJrmEB3em"
      },
      "source": [
        "> Are you satisfied with the error rate ? If no :\n",
        "  - Can we explain this high error rate in relation to the problem raised during the `Generating and analyzing train and test sets` section ?\n",
        "  - Try to address the problem."
      ]
    }
  ]
}